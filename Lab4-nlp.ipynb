{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab IV NLP - UTN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01'),\n",
       " Synset('frump.n.01'),\n",
       " Synset('dog.n.03'),\n",
       " Synset('cad.n.01'),\n",
       " Synset('frank.n.02'),\n",
       " Synset('pawl.n.01'),\n",
       " Synset('andiron.n.01'),\n",
       " Synset('chase.v.01')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dog')  #Todos los synsets de perro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('chase.v.01')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dog',pos=wn.VERB) #Solo los que lo toman como verbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('frump.n.01')\n",
      "Synset('frump.n.01')\n",
      "Synset('dog.n.01')\n"
     ]
    }
   ],
   "source": [
    "print(wn.synset('dog.n.02')) #dog.n.02 y frump.n.01 son el mismo elemento en el Synset\n",
    "print(wn.synset('frump.n.01'))\n",
    "print(wn.synset('dog.n.01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n",
      "\n",
      "a dull unattractive unpleasant girl or woman\n",
      "\n",
      "a dull unattractive unpleasant girl or woman\n"
     ]
    }
   ],
   "source": [
    "print(wn.synset('dog.n.01').definition())\n",
    "print(\"\\n\"+ wn.synset('dog.n.02').definition())\n",
    "print(\"\\n\"+ wn.synset('frump.n.01').definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('dog.n.01.dog'), Lemma('dog.n.01.domestic_dog'), Lemma('dog.n.01.Canis_familiaris')]\n",
      "[Lemma('frump.n.01.frump'), Lemma('frump.n.01.dog')]\n"
     ]
    }
   ],
   "source": [
    "print(wn.synset('dog.n.01').lemmas()) #Sinónimos\n",
    "print(wn.synset('dog.n.02').lemmas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dog.n.01').hypernyms() #Hiperónimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('basenji.n.01'),\n",
       " Synset('corgi.n.01'),\n",
       " Synset('cur.n.01'),\n",
       " Synset('dalmatian.n.02'),\n",
       " Synset('great_pyrenees.n.01'),\n",
       " Synset('griffon.n.02'),\n",
       " Synset('hunting_dog.n.01'),\n",
       " Synset('lapdog.n.01'),\n",
       " Synset('leonberg.n.01'),\n",
       " Synset('mexican_hairless.n.01'),\n",
       " Synset('newfoundland.n.01'),\n",
       " Synset('pooch.n.01'),\n",
       " Synset('poodle.n.01'),\n",
       " Synset('pug.n.01'),\n",
       " Synset('puppy.n.01'),\n",
       " Synset('spitz.n.01'),\n",
       " Synset('toy_dog.n.01'),\n",
       " Synset('working_dog.n.01')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dog.n.01').hyponyms() #Hipónimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('bad.a.01.bad')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good = wn.synset('good.a.01').lemmas()[0] #asignamos el primer lemma del synset\n",
    "good.antonyms() #Antónimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('flag.n.07')]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "dog = wn.synset('dog.n.01') #Sub-partes\n",
    "print(dog.part_meronyms()) #flag.n.07 es parte de dog\n",
    "print(dog.member_meronyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[Synset('canis.n.01'), Synset('pack.n.06')]\n"
     ]
    }
   ],
   "source": [
    "print(dog.part_holonyms()) #El TODO del cual dog es parte\n",
    "print(dog.member_holonyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a conspicuously marked or shaped tail\n",
      "type genus of the Canidae: domestic and wild dogs; wolves; jackals\n",
      "a group of hunting animals\n"
     ]
    }
   ],
   "source": [
    "print(wn.synset('flag.n.07').definition()) #cola, subparte del perro\n",
    "print(wn.synset('canis.n.01').definition()) #perro es holónimo de canino y de manada\n",
    "print(wn.synset('pack.n.06').definition()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intentaremos desambiguar la siguiente frase:\n",
    ">\"The bank can guarantee deposits will eventually cover future tuition costs because it invests\n",
    ">in adjustable-rate mortgage securities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('bank.n.01'),\n",
       " Synset('depository_financial_institution.n.01'),\n",
       " Synset('bank.n.03'),\n",
       " Synset('bank.n.04'),\n",
       " Synset('bank.n.05'),\n",
       " Synset('bank.n.06'),\n",
       " Synset('bank.n.07'),\n",
       " Synset('savings_bank.n.02'),\n",
       " Synset('bank.n.09'),\n",
       " Synset('bank.n.10')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('bank', pos=wn.NOUN)  #bank tiene 10 sentidos distintos para desambiguar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a financial institution that accepts deposits and channels the money into lending activities\n"
     ]
    }
   ],
   "source": [
    "print(wn.synset('depository_financial_institution.n.01').definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bank.n.05')\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk import word_tokenize\n",
    "S = \"The bank can guarantee deposits will eventually cover future tuition costs because it invests in adjustable-rate mortgage securities.\"\n",
    "S_tok = word_tokenize(S)\n",
    "print(lesk(S_tok, 'bank', 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a supply or stock held in reserve for future use (especially in emergencies)'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('bank.n.05').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'supply', 'or', 'stock', 'held', 'in', 'reserve', 'for', 'future', 'use', '(', 'especially', 'in', 'emergencies', ')']\n",
      "['a', 'financial', 'institution', 'that', 'accepts', 'deposits', 'and', 'channels', 'the', 'money', 'into', 'lending', 'activities']\n",
      "{'future', 'in'}\n",
      "{'deposits'}\n"
     ]
    }
   ],
   "source": [
    "l = word_tokenize((wn.synset('bank.n.05').definition()))\n",
    "print(l)\n",
    "m = word_tokenize((wn.synset('bank.n.02').definition()))\n",
    "print(m)\n",
    "k = set(S_tok)\n",
    "print(k.intersection(l)) \n",
    "print(k.intersection(m)) #Tiene mayor intersección con 'bank.n.05' por más que sea erróneo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué cambios, de haber alguno, sugeriría para la correspondencia implementada por NLTK?\n",
    "\n",
    "> Eliminaría las stopwords al 'in' ser parte de la interseccion de definiciones y no aportar nada, también las definiciones deberían estar armadas de una forma más descriptiva para poder ser mejor relacionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a container (usually with a slot in the top) for keeping money at home\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence = \"I went to the bank to deposit some money.\"\n",
    "sentence_tok = word_tokenize(sentence)#Wordnet: bank=depository financial institution\n",
    "print(lesk(sentence_tok, 'bank', 'n').definition()) # Miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a meal eaten in a mess hall by service personnel\n"
     ]
    }
   ],
   "source": [
    "sentence = \"She created a big mess of the birthday cake.\"\n",
    "sentence_tok = word_tokenize(sentence)#Wordnet: mess=a state of confusion and disorderliness\n",
    "print(lesk(sentence_tok, 'mess', 'n').definition()) # Miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the state of being certain that adverse effects will not be caused by some agent under defined conditions\n"
     ]
    }
   ],
   "source": [
    "sentence = \"In the interest of your safety, please wear your seatbelt.\"\n",
    "sentence_tok = word_tokenize(sentence) #Wordnet: safety=the state of being certain that adverse effects will...\n",
    "print(lesk(sentence_tok, 'safety', 'n').definition()) # Hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "having a low or inadequate temperature or feeling a sensation of coldness or having been made cold by e.g. ice or refrigeration\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I drank some ice cold water.\"\n",
    "sentence_tok = word_tokenize(sentence) #Wordnet: cold=having a low or inadequate temperature...\n",
    "print(lesk(sentence_tok, 'cold', 'a').definition()) # Hit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### *0,5 de presición en estas oraciones para el lesk*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "walk\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnLemmatizer = WordNetLemmatizer()\n",
    "print(wnLemmatizer.lemmatize('dogs', 'n')) \n",
    "print(wnLemmatizer.lemmatize('walking', 'v'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('went', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('bank', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('deposit', 'VB'),\n",
       " ('some', 'DT'),\n",
       " ('money', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(word_tokenize(\"I went to the bank to deposit some money.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio final: ¿Podrías mejorar el algoritmo de Lesk con un lematizador? Si es así, \n",
    "¿podrías escribir una función de match, que tome dos cadenas y devuelva las palabras\n",
    "coincidentes entre ellas? Queremos que las coincidencias(matches) sean útiles para un\n",
    "algoritmo como Lesk. ¿Quizás también necesites usar un etiquetador POS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "#map de tags a wordnet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def goodbye_stopwords(s1):\n",
    "    s1_tok = word_tokenize(s1)\n",
    "    filtered_sentence = [] \n",
    "    for w in s1_tok: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)  \n",
    "    return filtered_sentence\n",
    "    \n",
    "    \n",
    "def lem_pos_match(s1,s2):\n",
    "    #Takes in two sentences and does lemmatization + POS to match the words between them\n",
    "    #returns an array with the matched words e.g. [eat,run,goodbye]\n",
    "    s1 = s1.lower()\n",
    "    s2 = s2.lower()\n",
    "    s1_t = goodbye_stopwords(s1)\n",
    "    s2_t = goodbye_stopwords(s2)\n",
    "    s1_t_pos = nltk.pos_tag(s1_t)\n",
    "    s2_t_pos = nltk.pos_tag(s2_t)\n",
    "    s1_lem = [WordNetLemmatizer().lemmatize(t_pos[0],get_wordnet_pos(t_pos[1])) for t_pos in s1_t_pos]\n",
    "    s2_lem = [WordNetLemmatizer().lemmatize(t_pos[0],get_wordnet_pos(t_pos[1])) for t_pos in s2_t_pos]\n",
    "    return list(set(s1_lem).intersection(set(s2_lem)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['roast']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_pos_match(\"i like my eggs roasted\",\"i'm goning to roast in this heat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['roast']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_pos_match(\"i like my eggs roasted\",\"i'm goning to roast in this heat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kill', 'chill']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_pos_match(\"This cold is killing me and my coffee es chilling\",\"let's kill some time with netflix and chill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
